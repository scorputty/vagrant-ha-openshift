# This is an example of a bring your own (byo) host inventory
# https://github.com/openshift/openshift-ansible/blob/master/inventory/byo/hosts.example
# https://docs.openshift.org/latest/install_config/install/advanced_install.html
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
nodes
masters
etcd
lb
glusterfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]

# Proxy settings
openshift_http_proxy=http://10.0.2.2:3128
openshift_https_proxy=http://10.0.2.2:3128
openshift_no_proxy="localhost,127.0.0.1,.vagrant.test,192.168.100.0/24,172.30.0.0/16,10.128.0.0/14"

openshift_builddefaults_git_http_proxy=http://10.0.2.2:3128
openshift_builddefaults_git_https_proxy=http://10.0.2.2:3128
openshift_builddefaults_no_proxy="localhost,127.0.0.1,.vagrant.test,192.168.100.0/24,172.30.0.0/16,10.128.0.0/14"

# Debug level for all OpenShift components (Defaults to 2)
debug_level=4

# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=vagrant
# If ansible_ssh_user is not root, ansible_sudo must be set to True
ansible_sudo=True
ansible_become=yes

# Specify the deployment type. Valid values are origin and openshift-enterprise.
# deployment_type=origin
openshift_deployment_type=origin

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.6

# Specify an exact container image tag to install or configure.
# WARNING: This value will be used for all hosts in containerized environments, even those that have another version installed.
# This could potentially trigger an upgrade and downtime, so be careful with modifying this value after the cluster is set up.
#openshift_image_tag=v3.6.0

#(Pulling etcd is broken so for now False)
containerized=False

# This enables all the system containers except for docker:
openshift_use_system_containers=False
#
# But you can choose separately each component that must be a
# system container:
#
#openshift_use_openvswitch_system_container=False
#openshift_use_node_system_container=False
#openshift_use_master_system_container=False
#openshift_use_etcd_system_container=False
#
# In either case, system_images_registry must be specified to be able to find the system images
# system_images_registry="docker.io"
# when openshift_deployment_type=='openshift-enterprise'
#system_images_registry="registry.access.redhat.com"

# Manage openshift example imagestreams and templates during install and upgrade
openshift_install_examples=False

# Pre-Check settings
openshift_override_hostname_check=True
openshift_disable_check=disk_availability,memory_availability,docker_storage,docker_image_availability

# Master settings
# oscluster points to the "fake" external load balancer in our case gateway1.vagrant.test with DNS alias openshift-cluster.vagrant.test
openshift_master_cluster_method=native
openshift_master_cluster_hostname=console-openshift-cluster.vagrant.test
openshift_master_cluster_public_hostname=console-openshift-cluster.vagrant.test
openshift_master_default_subdomain=apps.openshift-cluster.vagrant.test
openshift_master_overwrite_named_certificates=True
openshift_master_api_port=8443
openshift_master_console_port=8443

# Networks
openshift_portal_net=172.30.0.0/16 # Each service in the cluster will be assigned an IP from this range.
osm_cluster_network_cidr=10.128.0.0/14 # Internal Pod's IP's

# Docker settings
openshift_docker_options='--log-driver json-file --log-opt max-size=1M --log-opt max-file=3 --selinux-enabled --insecure-registry 172.30.0.0/16'
openshift_docker_insecure_registries='172.30.0.0/16'

# Region settings
openshift_router_selector='region=infra'
openshift_registry_selector='region=infra'
osm_default_node_selector='region=primary'

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'True', 'challenge': 'True', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
# Generated like so:
# htpasswd -nb admin adm-password
# htpasswd -nb developer devel-password
openshift_master_htpasswd_users={'admin': '$apr1$6CZ4noKr$IksMFMgsW5e5FL0ioBhkk/', 'developer': '$apr1$AvisAPTG$xrVnJ/J0a83hAYlZcxHVf1'}


# Specify that we want to use GlusterFS storage for a hosted registry
# openshift_hosted_registry_storage_kind=glusterfs
# # Specify local GlusterFS and Heketi
openshift_storage_glusterfs_is_native=True
openshift_storage_glusterfs_heketi_is_native=True
openshift_storage_glusterfs_timeout=900
# # Set wipe to true only for reinstall
openshift_storage_glusterfs_wipe=True

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=True

# host group for masters
[masters]
osmaster1.vagrant.test openshift_ip=192.168.100.22 openshift_public_hostname=osmaster1.vagrant.test
osmaster2.vagrant.test openshift_ip=192.168.100.23 openshift_public_hostname=osmaster2.vagrant.test
osmaster3.vagrant.test openshift_ip=192.168.100.24 openshift_public_hostname=osmaster3.vagrant.test

# host group for etcd
[etcd]
osmaster1.vagrant.test openshift_ip=192.168.100.22
osmaster2.vagrant.test openshift_ip=192.168.100.23
osmaster3.vagrant.test openshift_ip=192.168.100.24

# host group load balancers
[lb]
gateway1.vagrant.test containerized=False

# host group for nodes, includes region info
[nodes]
# Master nodes
osmaster1.vagrant.test openshift_ip=192.168.100.22 openshift_node_labels="{'region': 'primary','zone': 'default'}" openshift_schedulable=False
osmaster2.vagrant.test openshift_ip=192.168.100.23 openshift_node_labels="{'region': 'primary','zone': 'default'}" openshift_schedulable=False
osmaster3.vagrant.test openshift_ip=192.168.100.24 openshift_node_labels="{'region': 'primary','zone': 'default'}" openshift_schedulable=False
# infra nodes
osinfra1.vagrant.test openshift_ip=192.168.100.32 openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=True
osinfra2.vagrant.test openshift_ip=192.168.100.33 openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=True
# app nodes
osnode1.vagrant.test openshift_ip=192.168.100.52 openshift_node_labels="{'region': 'primary', 'zone': 'green'}" openshift_schedulable=True
osnode2.vagrant.test openshift_ip=192.168.100.53 openshift_node_labels="{'region': 'primary', 'zone': 'blue'}" openshift_schedulable=True
# gluster nodes
gluster1.vagrant.test openshift_ip=192.168.100.12
gluster2.vagrant.test openshift_ip=192.168.100.13
gluster3.vagrant.test openshift_ip=192.168.100.14
# gluster4.vagrant.test openshift_ip=192.168.100.15
# # Specify the glusterfs group, which contains the nodes that will host
# # GlusterFS storage pods. At a minimum, each node must have a
# # "glusterfs_devices" variable defined. This variable is a list of block
# # devices the node will have access to that is intended solely for use as
# # GlusterFS storage. These block devices must be bare (e.g. have no data, not
# # be marked as LVM PVs), and will be formatted.
# # It is recommended to not use a single cluster for both general and registry
# # storage, so two three-node clusters will be required.
[glusterfs]
gluster1.vagrant.test glusterfs_ip=192.168.100.12 glusterfs_devices='[ "/dev/sdb", "/dev/sdc" ]'
gluster2.vagrant.test glusterfs_ip=192.168.100.13 glusterfs_devices='[ "/dev/sdb", "/dev/sdc" ]'
gluster3.vagrant.test glusterfs_ip=192.168.100.14 glusterfs_devices='[ "/dev/sdb", "/dev/sdc" ]'
# gluster4.vagrant.test glusterfs_ip=192.168.100.15 glusterfs_zone=2 glusterfs_devices='["/dev/sdb"]'
# [glusterfs_registry]
# gluster1.vagrant.test glusterfs_ip=192.168.100.12 glusterfs_devices='[ "/dev/sdb", "/dev/sdc" ]'
# gluster2.vagrant.test glusterfs_ip=192.168.100.13 glusterfs_devices='[ "/dev/sdb", "/dev/sdc" ]'
# gluster3.vagrant.test glusterfs_ip=192.168.100.14 glusterfs_devices='[ "/dev/sdb", "/dev/sdc" ]'
