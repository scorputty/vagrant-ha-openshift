# This is an example of a bring your own (byo) host inventory
# https://github.com/openshift/openshift-ansible/inventory/byo/hosts.example
# https://docs.openshift.org/latest/install_config/install/advanced_install.html
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
etcd
nodes
# lb
# glusterfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]

# Proxy settings
openshift_http_proxy=http://10.0.2.2:3128
openshift_https_proxy=http://10.0.2.2:3128
openshift_no_proxy="localhost,127.0.0.1,.vagrant.test,192.168.42.22,192.168.42.32,192.168.42.33,192.168.42.52,192.168.42.53,172.30.0.0/16,10.128.0.0/14"
# Most environments do not require a proxy between OpenShift masters, nodes, and
# etcd hosts. So automatically add those host names to the openshift_no_proxy list.
# If all of your hosts share a common domain you may wish to disable this and
# specify that domain above.
openshift_generate_no_proxy_hosts=True

# openshift_builddefaults_git_http_proxy=http://10.0.2.2:3128
# openshift_builddefaults_git_https_proxy=http://10.0.2.2:3128
# openshift_builddefaults_no_proxy="localhost,127.0.0.1,.vagrant.test,192.168.42.0/24,172.30.0.0/16,10.128.0.0/14"

# Debug level for all OpenShift components (Defaults to 2)
debug_level=4

# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=vagrant
# If ansible_ssh_user is not root, ansible_sudo must be set to True
ansible_sudo=True
ansible_become=yes

# Specify the deployment type. Valid values are origin and openshift-enterprise.
# deployment_type=origin
openshift_deployment_type=origin

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.7
openshift_image_tag=v3.7.0
# openshift_version=v3.7.0
# openshift_pkg_version=v3.7.0
# OpenShift repository configuration
#openshift_additional_repos=[{'id': 'openshift-origin-copr', 'name': 'OpenShift Origin COPR', 'baseurl': 'https://copr-be.cloud.fedoraproject.org/results/maxamillion/origin-next/epel-7-$basearch/', 'enabled': 1, 'gpgcheck': 1, 'gpgkey': 'https://copr-be.cloud.fedoraproject.org/results/maxamillion/origin-next/pubkey.gpg'}]
openshift_repos_enable_testing=True

# Specify an exact container image tag to install or configure.
# WARNING: This value will be used for all hosts in containerized environments, even those that have another version installed.
# This could potentially trigger an upgrade and downtime, so be careful with modifying this value after the cluster is set up.
#openshift_image_tag=v3.6.0

#(Pulling etcd is broken so for now False)
containerized=True

# This enables all the system containers except for docker:
openshift_use_system_containers=True
#
# But you can choose separately each component that must be a
# system container:
#
#openshift_use_openvswitch_system_container=False
#openshift_use_node_system_container=False
#openshift_use_master_system_container=False
#openshift_use_etcd_system_container=False
#
# In either case, system_images_registry must be specified to be able to find the system images
# system_images_registry="docker.io"
# when openshift_deployment_type=='openshift-enterprise'
#system_images_registry="registry.access.redhat.com"

# Manage openshift example imagestreams and templates during install and upgrade
openshift_install_examples=True

# Pre-Check settings
# openshift_override_hostname_check=True
openshift_disable_check=disk_availability,memory_availability,docker_storage,docker_image_availability

# Master settings
# oscluster points to the "fake" external load balancer in our case gateway1.vagrant.test with DNS alias openshift-cluster.vagrant.test
openshift_master_cluster_method=native
openshift_master_cluster_hostname=console-openshift-cluster.vagrant.test
openshift_master_cluster_public_hostname=console-openshift-cluster.vagrant.test
openshift_master_default_subdomain=apps.openshift-cluster.vagrant.test
# openshift_master_overwrite_named_certificates=True
openshift_master_api_port=8443
openshift_master_console_port=8443

# Networks
# Each service in the cluster will be assigned an IP from this range.
openshift_portal_net=172.30.0.0/16
# Internal Pod's IP's
osm_cluster_network_cidr=10.128.0.0/14

# Docker settings
# openshift_docker_options='--log-driver json-file --log-opt max-size=1M --log-opt max-file=3 --selinux-enabled --insecure-registry 172.30.0.0/16'
# openshift_docker_insecure_registries='172.30.0.0/16'
openshift_docker_options='--log-driver json-file --log-opt max-size=1M --log-opt max-file=3'

# Region settings

# openshift_router_selector='region=infra'
# openshift_registry_selector='region=infra'
# osm_default_node_selector='region=primary'

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'True', 'challenge': 'True', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
# Generated like so:
# htpasswd -nb admin adm-password
# htpasswd -nb developer devel-password
# Moved "openshift_master_htpasswd_users=" vault-vars.yml (for testing) vault password is test1234

# Specify that we want to use GlusterFS storage for a hosted registry
# openshift_hosted_registry_storage_kind=glusterfs
# # Specify local GlusterFS and Heketi
# openshift_storage_glusterfs_is_native=True
# openshift_storage_glusterfs_heketi_is_native=True
# openshift_storage_glusterfs_timeout=900
# # Set wipe to true only for reinstall
# openshift_storage_glusterfs_wipe=True

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=True

# host group for masters
[masters]
osmaster1.vagrant.test openshift_ip=192.168.42.22

# host group for etcd
[etcd]
osmaster1.vagrant.test openshift_ip=192.168.42.22

# host group load balancers
# [lb]
# gateway1.vagrant.test containerized=False

# host group for nodes, includes region info
[nodes]
# Master nodes
osmaster1.vagrant.test openshift_ip=192.168.42.22 openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=True
# infra nodes
# osinfra1.vagrant.test openshift_ip=192.168.42.32 openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
# osinfra2.vagrant.test openshift_ip=192.168.42.33 openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
# app nodes
osnode1.vagrant.test openshift_ip=192.168.42.52 openshift_node_labels="{'region': 'primary', 'zone': 'green'}"
osnode2.vagrant.test openshift_ip=192.168.42.53 openshift_node_labels="{'region': 'primary', 'zone': 'blue'}"
